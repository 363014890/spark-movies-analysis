{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5003 project code\n",
    "\n",
    "#### In this projec, we implemented pair programming through vscode live share. Thus, we tried the findspark to start our spark in vscode below. We shoud install findspark first.\n",
    "\n",
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /Users/zhanghousu/conda/lib/python3.9/site-packages (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 22:28:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/24 22:28:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/04/24 22:28:38 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init(spark_home='/Users/zhanghousu/spark/spark-3.3.1-bin-hadoop2',\n",
    "                python_path='/Users/zhanghousu/conda/bin/python')\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the files in RDD\n",
    "credits = sc.textFile('movie/credits.csv')\n",
    "links = sc.textFile('movie/links.csv')\n",
    "ratings = sc.textFile('movie/ratings.csv')\n",
    "movies_metadata = sc.textFile('movie/movies_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 22:28:44 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 22:28:52 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 4 (TID 10): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 22:29:05 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 6 (TID 33): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession(sc)\n",
    "cred_df = spark.read.csv(credits, header=True, inferSchema=True, escape='\"')\n",
    "links_df = spark.read.csv(links, header=True, inferSchema=True, escape='\"')\n",
    "ratings_df = spark.read.csv(ratings, header=True, inferSchema=True, escape='\"')\n",
    "movies_metadata_df = spark.read.csv(movies_metadata, header=True, inferSchema=True, escape='\"')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleanning credit dataset\n",
    "#### calculating null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=========>                                                 (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+\n",
      "|cast|crew| id|\n",
      "+----+----+---+\n",
      "|   0|   0|  0|\n",
      "+----+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calucate the null values in each column\n",
    "null_rows = cred_df.select(*[(sum(when(isnull(c), 1).otherwise(0))).alias(c) for c in cred_df.columns])\n",
    "null_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+\n",
      "|cast|crew| id|\n",
      "+----+----+---+\n",
      "|   0|   0|  0|\n",
      "+----+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# drop the rows with null values\n",
    "cred_df_n = cred_df.dropna()\n",
    "null_rows_check = cred_df_n.select(*[(sum(when(isnull(c), 1).otherwise(0))).alias(c) for c in cred_df_n.columns])\n",
    "null_rows_check.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleanning metadata dataset\n",
    "#### removing useless features, calculating null data, data StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45572"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop useless columns\n",
    "mv_meta_df_dp = movies_metadata_df.drop('adult', 'belongs_to_collection', 'homepage', 'imdb_id', \n",
    "                                        'original_title', 'overview', 'poster_path', 'release_date', \n",
    "                                        'spoken_languages', 'title', 'tagline', 'video')\n",
    "\n",
    "# show the size of the mv_meta_df_dp\n",
    "mv_meta_df_dp.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44941"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the rows with status not equal to \"Released\"\n",
    "mv_meta_df_dp = mv_meta_df_dp.filter(col(\"status\") == \"Released\")\n",
    "mv_meta_df_dp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 22:29:13 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 20 (TID 56): Attempting to kill Python Worker\n",
      "+--------+--------------------+-----+-----------------+----------+--------------------+--------------------+---------+-------+------------+----------+\n",
      "|  budget|              genres|   id|original_language|popularity|production_companies|production_countries|  revenue|runtime|vote_average|vote_count|\n",
      "+--------+--------------------+-----+-----------------+----------+--------------------+--------------------+---------+-------+------------+----------+\n",
      "|30000000|[{'id': 16, 'name...|  862|               en| 21.946943|[{'name': 'Pixar ...|[{'iso_3166_1': '...|373554033|   81.0|         7.7|      5415|\n",
      "|65000000|[{'id': 12, 'name...| 8844|               en| 17.015539|[{'name': 'TriSta...|[{'iso_3166_1': '...|262797249|  104.0|         6.9|      2413|\n",
      "|       0|[{'id': 10749, 'n...|15602|               en|   11.7129|[{'name': 'Warner...|[{'iso_3166_1': '...|        0|  101.0|         6.5|        92|\n",
      "|16000000|[{'id': 35, 'name...|31357|               en|  3.859495|[{'name': 'Twenti...|[{'iso_3166_1': '...| 81452156|  127.0|         6.1|        34|\n",
      "|       0|[{'id': 35, 'name...|11862|               en|  8.387519|[{'name': 'Sandol...|[{'iso_3166_1': '...| 76578911|  106.0|         5.7|       173|\n",
      "+--------+--------------------+-----+-----------------+----------+--------------------+--------------------+---------+-------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# remove status column\n",
    "mv_meta_df_dp = mv_meta_df_dp.drop('status')\n",
    "mv_meta_df_dp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34065"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the rows with budget and revenue equal to 0\n",
    "mv_meta_df_dp.filter((col(\"budget\") == 0) & (col(\"revenue\") == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 22:29:18 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 24 (TID 60): Attempting to kill Python Worker\n",
      "+--------------------+-----+-----------------+----------+--------------------+--------------------+-------+------------+----------+\n",
      "|              genres|   id|original_language|popularity|production_companies|production_countries|runtime|vote_average|vote_count|\n",
      "+--------------------+-----+-----------------+----------+--------------------+--------------------+-------+------------+----------+\n",
      "|[{'id': 16, 'name...|  862|               en| 21.946943|[{'name': 'Pixar ...|[{'iso_3166_1': '...|   81.0|         7.7|      5415|\n",
      "|[{'id': 12, 'name...| 8844|               en| 17.015539|[{'name': 'TriSta...|[{'iso_3166_1': '...|  104.0|         6.9|      2413|\n",
      "|[{'id': 10749, 'n...|15602|               en|   11.7129|[{'name': 'Warner...|[{'iso_3166_1': '...|  101.0|         6.5|        92|\n",
      "|[{'id': 35, 'name...|31357|               en|  3.859495|[{'name': 'Twenti...|[{'iso_3166_1': '...|  127.0|         6.1|        34|\n",
      "|[{'id': 35, 'name...|11862|               en|  8.387519|[{'name': 'Sandol...|[{'iso_3166_1': '...|  106.0|         5.7|       173|\n",
      "+--------------------+-----+-----------------+----------+--------------------+--------------------+-------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mv_meta_df_dp = mv_meta_df_dp.drop('budget', 'revenue')\n",
    "mv_meta_df_dp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genres: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      " |-- vote_average: string (nullable = true)\n",
      " |-- vote_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mv_meta_df_dp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genres: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# create DoubleType list\n",
    "columns_to_convert = [\"popularity\", \"runtime\", \"vote_average\", \"vote_count\"]\n",
    "\n",
    "# convert the columns to DoubleType\n",
    "for column in columns_to_convert:\n",
    "    mv_meta_df_dp = mv_meta_df_dp.withColumn(column, mv_meta_df_dp[column].cast(DoubleType()))\n",
    "\n",
    "# check the schema\n",
    "mv_meta_df_dp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 22:29:25 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 37 (TID 73): Attempting to kill Python Worker\n",
      "+--------------------+-----+-----------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|              genres|   id|original_language|         popularity|production_companies|production_countries|            runtime|       vote_average|          vote_count|\n",
      "+--------------------+-----+-----------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|[{'id': 16, 'name...|  862|               en| 3.1532246689818866|[{'name': 'Pixar ...|[{'iso_3166_1': '...|-0.3460900547748043| 1.0839043917589501|  10.736622418900206|\n",
      "|[{'id': 12, 'name...| 8844|               en| 2.3350472627476364|[{'name': 'TriSta...|[{'iso_3166_1': '...|  0.253392902201182| 0.6661104271252892|   4.659792962045657|\n",
      "|[{'id': 10749, 'n...|15602|               en| 1.4552776427334224|[{'name': 'Warner...|[{'iso_3166_1': '...|0.17519947303040118|0.45721344480845855|-0.03851522228459...|\n",
      "|[{'id': 35, 'name...|31357|               en|0.15230621045833603|[{'name': 'Twenti...|[{'iso_3166_1': '...| 0.8528758591771682|0.24831646249162787|-0.15592232038504694|\n",
      "|[{'id': 35, 'name...|11862|               en| 0.9035581727162311|[{'name': 'Sandol...|[{'iso_3166_1': '...|0.30552185498170253|0.03941948017479766| 0.12544986299362806|\n",
      "+--------------------+-----+-----------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, stddev, col\n",
    "\n",
    "# create a list of columns to normalize\n",
    "for column in columns_to_convert:\n",
    "    # calculate the mean and standard deviation\n",
    "    mean_value, stddev_value = mv_meta_df_dp.select(mean(col(column)), stddev(col(column))).first()\n",
    "\n",
    "    # normalize the column\n",
    "    mv_meta_df_dp = mv_meta_df_dp.withColumn(column, (col(column) - mean_value) / stddev_value)\n",
    "\n",
    "mv_meta_df_dp.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# create a new column named \"profit\"\n",
    "def extract_first_genre(genres_str):\n",
    "    try:\n",
    "        genres_list = ast.literal_eval(genres_str)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return 'no genre'\n",
    "    \n",
    "    if len(genres_str) <= 2:\n",
    "        return 'no genre'\n",
    "    else:\n",
    "        genres_list = ast.literal_eval(genres_str)\n",
    "        return genres_list[0]['name']\n",
    "\n",
    "\n",
    "# create a user defined function\n",
    "first_genre_udf = udf(extract_first_genre, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 22:29:29 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 38 (TID 74): Attempting to kill Python Worker\n",
      "+--------------------+-----+-----------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+-----------+\n",
      "|              genres|   id|original_language|         popularity|production_companies|production_countries|            runtime|       vote_average|          vote_count|first_genre|\n",
      "+--------------------+-----+-----------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+-----------+\n",
      "|[{'id': 16, 'name...|  862|               en| 3.1532246689818866|[{'name': 'Pixar ...|[{'iso_3166_1': '...|-0.3460900547748043| 1.0839043917589501|  10.736622418900206|  Animation|\n",
      "|[{'id': 12, 'name...| 8844|               en| 2.3350472627476364|[{'name': 'TriSta...|[{'iso_3166_1': '...|  0.253392902201182| 0.6661104271252892|   4.659792962045657|  Adventure|\n",
      "|[{'id': 10749, 'n...|15602|               en| 1.4552776427334224|[{'name': 'Warner...|[{'iso_3166_1': '...|0.17519947303040118|0.45721344480845855|-0.03851522228459...|    Romance|\n",
      "|[{'id': 35, 'name...|31357|               en|0.15230621045833603|[{'name': 'Twenti...|[{'iso_3166_1': '...| 0.8528758591771682|0.24831646249162787|-0.15592232038504694|     Comedy|\n",
      "|[{'id': 35, 'name...|11862|               en| 0.9035581727162311|[{'name': 'Sandol...|[{'iso_3166_1': '...|0.30552185498170253|0.03941948017479766| 0.12544986299362806|     Comedy|\n",
      "+--------------------+-----+-----------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# apply the udf to the dataframe\n",
    "mv_meta_df_oh = mv_meta_df_dp.withColumn(\"first_genre\", first_genre_udf(col(\"genres\")))\n",
    "mv_meta_df_oh.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- runtime: double (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: double (nullable = true)\n",
      " |-- first_genre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mv_meta_df_oh = mv_meta_df_oh.drop('genres')\n",
    "mv_meta_df_oh.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_meta_df_oh.select(\"first_genre\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# create a StringIndexer and OneHotEncoder\n",
    "indexer = StringIndexer(inputCol=\"first_genre\", outputCol=\"first_genre_index\")\n",
    "encoder = OneHotEncoder(inputCol=\"first_genre_index\", outputCol=\"first_genre_onehot\", dropLast=False)\n",
    "\n",
    "# create a pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder])\n",
    "\n",
    "# fit the pipeline to the dataframe\n",
    "model = pipeline.fit(mv_meta_df_oh)\n",
    "mv_meta_df_oh_encoded = model.transform(mv_meta_df_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a function to convert a vector to an array\n",
    "def vector_to_array(vec):\n",
    "    if isinstance(vec, SparseVector):\n",
    "        vec = DenseVector(vec)\n",
    "    return vec.toArray().tolist()\n",
    "\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(DoubleType()))\n",
    "\n",
    "# apply the function to the dataframe\n",
    "mv_meta_df_oh_encoded = mv_meta_df_oh_encoded.withColumn(\"first_genre_onehot_array\", vector_to_array_udf(col(\"first_genre_onehot\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|   id|original_language|         popularity|production_companies|production_countries|            runtime|       vote_average|          vote_count|FG_onehot_0|FG_onehot_1|FG_onehot_2|FG_onehot_3|FG_onehot_4|FG_onehot_5|FG_onehot_6|FG_onehot_7|FG_onehot_8|FG_onehot_9|FG_onehot_10|FG_onehot_11|FG_onehot_12|FG_onehot_13|FG_onehot_14|FG_onehot_15|FG_onehot_16|FG_onehot_17|FG_onehot_18|FG_onehot_19|FG_onehot_20|\n",
      "+-----+-----------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|  862|               en| 3.1532246689818866|[{'name': 'Pixar ...|[{'iso_3166_1': '...|-0.3460900547748043| 1.0839043917589501|  10.736622418900206|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         1.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "| 8844|               en| 2.3350472627476364|[{'name': 'TriSta...|[{'iso_3166_1': '...|  0.253392902201182| 0.6661104271252892|   4.659792962045657|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        1.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|15602|               en| 1.4552776427334224|[{'name': 'Warner...|[{'iso_3166_1': '...|0.17519947303040118|0.45721344480845855|-0.03851522228459...|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        1.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|31357|               en|0.15230621045833603|[{'name': 'Twenti...|[{'iso_3166_1': '...| 0.8528758591771682|0.24831646249162787|-0.15592232038504694|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|11862|               en| 0.9035581727162311|[{'name': 'Sandol...|[{'iso_3166_1': '...|0.30552185498170253|0.03941948017479766| 0.12544986299362806|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "+-----+-----------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get the number of categories\n",
    "num_categories = len(model.stages[0].labels)\n",
    "for i in range(num_categories):\n",
    "    mv_meta_df_oh_encoded = mv_meta_df_oh_encoded.withColumn(\n",
    "        f\"FG_onehot_{i}\",\n",
    "        mv_meta_df_oh_encoded[\"first_genre_onehot_array\"].getItem(i)\n",
    "    )\n",
    "\n",
    "# drop the original columns\n",
    "mv_meta_df_oh_encoded = mv_meta_df_oh_encoded.drop(\"first_genre_onehot_array\", \"first_genre_onehot\", \n",
    "                                                   \"first_genre\", \"first_genre_index\")\n",
    "mv_meta_df_oh_encoded.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44941"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_meta_df_oh_encoded.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column named production_companies\n",
    "def extract_first_prod(prod_str):\n",
    "    try:\n",
    "        prod_list = ast.literal_eval(prod_str)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return 'no production company'\n",
    "\n",
    "    if len(prod_str) <= 2:\n",
    "        return 'no production company'\n",
    "    else:\n",
    "        prod_list = ast.literal_eval(prod_str)\n",
    "        return prod_list[0]['name']\n",
    "\n",
    "\n",
    "# create a user defined function\n",
    "first_prod_udf = udf(extract_first_prod, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10543"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the udf to the dataframe\n",
    "mv_meta_df_prod = mv_meta_df_dp.withColumn(\"first_production\", first_prod_udf(col(\"production_companies\")))\n",
    "mv_meta_df_prod.select(\"first_production\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|   id|         popularity|            runtime|       vote_average|          vote_count|FG_onehot_0|FG_onehot_1|FG_onehot_2|FG_onehot_3|FG_onehot_4|FG_onehot_5|FG_onehot_6|FG_onehot_7|FG_onehot_8|FG_onehot_9|FG_onehot_10|FG_onehot_11|FG_onehot_12|FG_onehot_13|FG_onehot_14|FG_onehot_15|FG_onehot_16|FG_onehot_17|FG_onehot_18|FG_onehot_19|FG_onehot_20|\n",
      "+-----+-------------------+-------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|  862| 3.1532246689818866|-0.3460900547748043| 1.0839043917589501|  10.736622418900206|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         1.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "| 8844| 2.3350472627476364|  0.253392902201182| 0.6661104271252892|   4.659792962045657|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        1.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|15602| 1.4552776427334224|0.17519947303040118|0.45721344480845855|-0.03851522228459...|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        1.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|31357|0.15230621045833603| 0.8528758591771682|0.24831646249162787|-0.15592232038504694|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|11862| 0.9035581727162311|0.30552185498170253|0.03941948017479766| 0.12544986299362806|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "+-----+-------------------+-------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# too many categories of the production\n",
    "mv_meta_df_oh_encoded = mv_meta_df_oh_encoded.drop('production_companies', 'original_language', \n",
    "                                   'spoken_languages', 'production_companies',\n",
    "                                   'production_countries')\n",
    "mv_meta_df_oh_encoded.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleanning the credits dataset\n",
    "#### obtain the first three casts and calculate a new avg rating table for the casts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|castId|\n",
      "+-------+------+\n",
      "|    862|    31|\n",
      "|    862| 12898|\n",
      "|    862|  7167|\n",
      "|   8844|  2157|\n",
      "|   8844|  8537|\n",
      "+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, explode,\\\n",
    "    get_json_object, regexp_replace, array\n",
    "from pyspark.sql.types import StructType, StructField,\\\n",
    "    StringType, IntegerType, ArrayType, MapType, DoubleType\n",
    "\n",
    "# read the data\n",
    "cred_df = spark.read.csv('movie/credits.csv', header=True, escape='\"')\n",
    "movi_df = spark.read.csv('movie/movies_metadata.csv', header=True, escape='\"')\n",
    "\n",
    "# remove the rows with budget and revenue equal to 0\n",
    "json_schema = ArrayType(StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "]))\n",
    "\n",
    "# remove the rows with budget and revenue equal to 0\n",
    "cred_df = cred_df.withColumn('cast',regexp_replace(\"cast\", 'None', \"'None'\"))\n",
    "cred_df = cred_df.withColumn(\"cast_array\", from_json(col(\"cast\"),json_schema))\n",
    "cred_df = cred_df.withColumn(\"cast_array\",\\\n",
    "                             array(col('cast_array')[0],\\\n",
    "                                   col('cast_array')[1],\\\n",
    "                                   col('cast_array')[2]))\n",
    "\n",
    "cast_df = cred_df.select(col(\"id\").alias(\"movieId\"), explode(col(\"cast_array\"))\\\n",
    "                         .alias(\"cast_item\"))\n",
    "cast_name_df = cast_df.withColumn(\"castId\", col(\"cast_item\")[\"id\"])\\\n",
    "                    .drop('cast_item')\n",
    "\n",
    "cast_name_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "| castId|            rating|\n",
      "+-------+------------------+\n",
      "|1362204|             3.375|\n",
      "|  18979|3.7976480836236934|\n",
      "|   1959| 3.250567650909128|\n",
      "|  69637|3.7530362663626198|\n",
      "| 107536|2.9383561643835616|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove the rows with budget and revenue equal to 0\n",
    "ratings_df = spark.read.csv('movie/ratings.csv', header=True, escape='\"')\n",
    "ratings_df = ratings_df.withColumn('rating', ratings_df['rating']\\\n",
    "                                   .cast(DoubleType()))\n",
    "ratings_df = ratings_df.groupBy(\"movieId\").avg(\"rating\")\\\n",
    "                        .withColumnRenamed(\"avg(rating)\", \"rating\")\n",
    "\n",
    "cast_name_with_rating = cast_name_df\\\n",
    "                        .join(ratings_df,\\\n",
    "                        cast_name_df.movieId == ratings_df.movieId,\"inner\")\\\n",
    "                        .groupBy(\"castId\")\\\n",
    "                        .avg(\"rating\")\\\n",
    "                        .withColumnRenamed(\"avg(rating)\", \"rating\")\n",
    "cast_name_with_rating.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|movieId|   avg_cast_rating|\n",
      "+-------+------------------+\n",
      "| 141418|             3.375|\n",
      "|  55321| 3.203446833591777|\n",
      "|  47880|3.0102890144060197|\n",
      "|  47940|3.3937500000000003|\n",
      "| 356054| 3.208588957055215|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# join the two dataframes\n",
    "joined_df = cast_name_df.join(cast_name_with_rating, on=\"castId\", how=\"inner\")\n",
    "\n",
    "# calculate the average rating for each movie\n",
    "mv_cast_avg_rating = joined_df.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"avg_cast_rating\"))\n",
    "\n",
    "# join the two dataframes\n",
    "mv_cast_avg_rating.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33125"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_cast_avg_rating.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|movieId|         popularity|            runtime|       vote_average|          vote_count|FG_onehot_0|FG_onehot_1|FG_onehot_2|FG_onehot_3|FG_onehot_4|FG_onehot_5|FG_onehot_6|FG_onehot_7|FG_onehot_8|FG_onehot_9|FG_onehot_10|FG_onehot_11|FG_onehot_12|FG_onehot_13|FG_onehot_14|FG_onehot_15|FG_onehot_16|FG_onehot_17|FG_onehot_18|FG_onehot_19|FG_onehot_20|\n",
      "+-------+-------------------+-------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|    862| 3.1532246689818866|-0.3460900547748043| 1.0839043917589501|  10.736622418900206|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         1.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|   8844| 2.3350472627476364|  0.253392902201182| 0.6661104271252892|   4.659792962045657|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        1.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|  15602| 1.4552776427334224|0.17519947303040118|0.45721344480845855|-0.03851522228459...|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        1.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|  31357|0.15230621045833603| 0.8528758591771682|0.24831646249162787|-0.15592232038504694|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "|  11862| 0.9035581727162311|0.30552185498170253|0.03941948017479766| 0.12544986299362806|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|\n",
      "+-------+-------------------+-------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# rename the it to movieId\n",
    "mv_meta_df_oh_encoded = mv_meta_df_oh_encoded.withColumnRenamed(\"id\", \"movieId\")\n",
    "mv_meta_df_oh_encoded.drop(\"id\")\n",
    "mv_meta_df_oh_encoded.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------------+\n",
      "|movieId|          popularity|             runtime|      vote_average|          vote_count|FG_onehot_0|FG_onehot_1|FG_onehot_2|FG_onehot_3|FG_onehot_4|FG_onehot_5|FG_onehot_6|FG_onehot_7|FG_onehot_8|FG_onehot_9|FG_onehot_10|FG_onehot_11|FG_onehot_12|FG_onehot_13|FG_onehot_14|FG_onehot_15|FG_onehot_16|FG_onehot_17|FG_onehot_18|FG_onehot_19|FG_onehot_20|   avg_cast_rating|\n",
      "+-------+--------------------+--------------------+------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------------+\n",
      "| 141418|-0.39098362011825155|-0.24183214921376323|1.1361286373381574|-0.20855308849904372|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         1.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|             3.375|\n",
      "|  55321| -0.4816785634667032| 0.38371528415248335|-2.937362517840037| -0.2227229106835813|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0| 3.203446833591777|\n",
      "|  47880|-0.04036616661172418|  0.3576508077622231|0.6138861815460813| -0.2045045678748901|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|3.0102890144060197|\n",
      "|  47940|-0.29958189311334876|  2.0257772967388807|0.8750074094421194|-0.19843178693865973|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|3.3937500000000003|\n",
      "| 356054| -0.4877555756189317| -0.5285413895066262|-2.937362517840037| -0.2247471709956581|        0.0|        0.0|        0.0|        0.0|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0| 3.208588957055215|\n",
      "+-------+--------------------+--------------------+------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# join the two dataframes\n",
    "main = mv_meta_df_oh_encoded.join(mv_cast_avg_rating, on=\"movieId\", how=\"inner\")\n",
    "main.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45115"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32828"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the main and ratings dataframe\n",
    "main = main.join(ratings_df, on=\"movieId\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7231"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main = main.drop(\"movieId\")\n",
    "main.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------------+------------------+\n",
      "|          popularity|             runtime|       vote_average|          vote_count|FG_onehot_0|FG_onehot_1|FG_onehot_2|FG_onehot_3|FG_onehot_4|FG_onehot_5|FG_onehot_6|FG_onehot_7|FG_onehot_8|FG_onehot_9|FG_onehot_10|FG_onehot_11|FG_onehot_12|FG_onehot_13|FG_onehot_14|FG_onehot_15|FG_onehot_16|FG_onehot_17|FG_onehot_18|FG_onehot_19|FG_onehot_20|   avg_cast_rating|            rating|\n",
      "+--------------------+--------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------------+------------------+\n",
      "|-0.39098362011825155|-0.24183214921376323| 1.1361286373381574|-0.20855308849904372|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         1.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|             3.375|             3.375|\n",
      "|-0.04036616661172418|  0.3576508077622231| 0.6138861815460813| -0.2045045678748901|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|3.0102890144060197|            2.8125|\n",
      "|-0.29958189311334876|  2.0257772967388807| 0.8750074094421194|-0.19843178693865973|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|3.3937500000000003|            3.3125|\n",
      "|  2.0236835116858054|  0.8789403355674286| 0.6661104271252892|   4.014053922493158|        0.0|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|3.1146137026307215| 3.829846582984658|\n",
      "|   1.137294348402791|  0.1491349966401409|0.03941948017479766|-0.15187379976089335|        0.0|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|2.8911336469815705|2.5669096589429836|\n",
      "+--------------------+--------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import when, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "threshold = 3.0  # Replace with the threshold you want to use for classification\n",
    "main = main.withColumn(\"label\", when(col(\"rating\") > threshold, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------------+-----+\n",
      "|          popularity|             runtime|       vote_average|          vote_count|FG_onehot_0|FG_onehot_1|FG_onehot_2|FG_onehot_3|FG_onehot_4|FG_onehot_5|FG_onehot_6|FG_onehot_7|FG_onehot_8|FG_onehot_9|FG_onehot_10|FG_onehot_11|FG_onehot_12|FG_onehot_13|FG_onehot_14|FG_onehot_15|FG_onehot_16|FG_onehot_17|FG_onehot_18|FG_onehot_19|FG_onehot_20|   avg_cast_rating|label|\n",
      "+--------------------+--------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------------+-----+\n",
      "|-0.39098362011825155|-0.24183214921376323| 1.1361286373381574|-0.20855308849904372|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         1.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|             3.375|    1|\n",
      "|-0.04036616661172418|  0.3576508077622231| 0.6138861815460813| -0.2045045678748901|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|3.0102890144060197|    0|\n",
      "|-0.29958189311334876|  2.0257772967388807| 0.8750074094421194|-0.19843178693865973|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|3.3937500000000003|    1|\n",
      "|  2.0236835116858054|  0.8789403355674286| 0.6661104271252892|   4.014053922493158|        0.0|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|3.1146137026307215|    1|\n",
      "|   1.137294348402791|  0.1491349966401409|0.03941948017479766|-0.15187379976089335|        0.0|        0.0|        1.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|2.8911336469815705|    0|\n",
      "+--------------------+--------------------+-------------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = main.drop(\"rating\")\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'label' is your label column\n",
    "label_column = 'label'\n",
    "\n",
    "# Get a list of all columns\n",
    "all_columns = final_df.columns\n",
    "\n",
    "# Remove the label column from the list of feature columns\n",
    "feature_columns = [column for column in all_columns if column != label_column]\n",
    "final_df = final_df.na.drop(subset=feature_columns)\n",
    "\n",
    "\n",
    "# Now feature_columns contains the names of all feature columns in your DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for the pipeline\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline_lr = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 22:44:53 WARN CacheManager: Asked to cache already cached data.\n",
      "23/04/24 22:44:53 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "crossval_lr = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  \n",
    "\n",
    "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "cv_model_lr = crossval_lr.fit(train_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(metricName='f1')\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "evaluator_recall = MulticlassClassificationEvaluator(metricName='weightedRecall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression test set accuracy:  0.6267247639796659\n",
      "Logistic Regression test set F1 score:  0.4829138136736176\n",
      "Logistic Regression test set recall:  0.6267247639796659\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "predictions_lr = cv_model_lr.transform(test_data)\n",
    "accuracy_lr = evaluator_accuracy.evaluate(predictions_lr)\n",
    "f1_score_lr = evaluator_f1.evaluate(predictions_lr)\n",
    "recall_lr = evaluator_recall.evaluate(predictions_lr)\n",
    "print(\"Logistic Regression test set accuracy: \", accuracy_lr)\n",
    "print(\"Logistic Regression test set F1 score: \", f1_score_lr)\n",
    "print(\"Logistic Regression test set recall: \", recall_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine test set accuracy:  0.869281045751634\n",
      "Support Vector Machine test set F1 score:  0.8678482291075018\n",
      "Support Vector Machine test set recall:  0.869281045751634\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Prepare for the pipeline\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "svm = LinearSVC(maxIter=10)\n",
    "pipeline_svm = Pipeline(stages=[assembler, svm])\n",
    "\n",
    "paramGrid_svm = ParamGridBuilder() \\\n",
    "    .addGrid(svm.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(svm.fitIntercept, [False, True]) \\\n",
    "    .build()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "crossval_svm = CrossValidator(estimator=pipeline_svm,\n",
    "                          estimatorParamMaps=paramGrid_svm,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  \n",
    "\n",
    "cv_model_svm = crossval_svm.fit(train_data)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found.\n",
    "predictions_svm = cv_model_svm.transform(test_data)\n",
    "accuracy_svm = evaluator_accuracy.evaluate(predictions_svm)\n",
    "f1_score_svm = evaluator_f1.evaluate(predictions_svm)\n",
    "recall_svm = evaluator_recall.evaluate(predictions_svm)\n",
    "print(\"Support Vector Machine test set accuracy: \", accuracy_svm)\n",
    "print(\"Support Vector Machine test set F1 score: \", f1_score_svm)\n",
    "print(\"Support Vector Machine test set recall: \", recall_svm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree test set accuracy:  0.8707334785766159\n",
      "Decision Tree test set F1 score:  0.8706304459671901\n",
      "Decision Tree test set recall:  0.8707334785766159\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Prepare for the pipeline\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "dt = DecisionTreeClassifier()\n",
    "pipeline_dt = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "paramGrid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [3, 4, 5]) \\\n",
    "    .addGrid(dt.impurity, [\"gini\", \"entropy\"]) \\\n",
    "    .build()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "crossval_dt = CrossValidator(estimator=pipeline_dt,\n",
    "                          estimatorParamMaps=paramGrid_dt,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  \n",
    "\n",
    "cv_model_dt = crossval_dt.fit(train_data)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found.\n",
    "predictions_dt = cv_model_dt.transform(test_data)\n",
    "accuracy_dt = evaluator_accuracy.evaluate(predictions_dt)\n",
    "f1_score_dt = evaluator_f1.evaluate(predictions_dt)\n",
    "recall_dt = evaluator_recall.evaluate(predictions_dt)\n",
    "print(\"Decision Tree test set accuracy: \", accuracy_dt)\n",
    "print(\"Decision Tree test set F1 score: \", f1_score_dt)\n",
    "print(\"Decision Tree test set recall: \", recall_dt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stop the spark session\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

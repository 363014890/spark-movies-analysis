{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb76b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 16:39:57 WARN Utils: Your hostname, MacBook-Pro-279.local resolves to a loopback address: 127.0.0.1; using 10.79.140.225 instead (on interface en0)\n",
      "23/04/29 16:39:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 16:39:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/29 16:39:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/29 16:39:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/04/29 16:39:59 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/04/29 16:39:59 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init(spark_home='/Users/zhanghousu/spark/spark-3.3.1-bin-hadoop2',\n",
    "                python_path='/Users/zhanghousu/conda/bin/python')\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sc.addPyFile(\"jars/graphframes-0.8.2-spark2.4-s_2.11.jar\")\n",
    "\n",
    "                \n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c7b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = sc.textFile('movie/links.csv')\n",
    "ratings = sc.textFile('movie/ratings.csv')\n",
    "movies_metadata = sc.textFile('movie/movies_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2fcace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 16:40:13 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 2 (TID 3): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 16:40:26 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 4 (TID 26): Attempting to kill Python Worker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "links_df = spark.read.csv(links, header=True, inferSchema=True, escape='\"')\n",
    "ratings_df = spark.read.csv(ratings, header=True, inferSchema=True, escape='\"')\n",
    "movies_metadata_df = spark.read.csv(movies_metadata, header=True, inferSchema=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465b27b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 16:42:22 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 7 (TID 30): Attempting to kill Python Worker\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    110|   1.0|\n",
      "|     1|    147|   4.5|\n",
      "|     1|    858|   5.0|\n",
      "|     1|   1221|   5.0|\n",
      "|     1|   1246|   5.0|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ra_id_rating = ratings_df.select('userId', 'movieId', 'rating')\n",
    "ra_id_rating.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "216de907",
   "metadata": {},
   "source": [
    "## This dataframe code will:\n",
    "\n",
    "#### Create a GraphFrame using the ra_id_rating DataFrame.\n",
    "#### Show the first 5 vertices and edges.\n",
    "#### Calculate the in-degree and out-degree of each vertex.\n",
    "#### Find the top 5 users who have rated the most movies.\n",
    "#### Find the top 5 movies with the most ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd901a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghousu/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|148|\n",
      "|463|\n",
      "|471|\n",
      "|496|\n",
      "|833|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 16:45:08 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 13 (TID 76): Attempting to kill Python Worker\n",
      "+---+----+------+\n",
      "|src| dst|rating|\n",
      "+---+----+------+\n",
      "|  1| 110|   1.0|\n",
      "|  1| 147|   4.5|\n",
      "|  1| 858|   5.0|\n",
      "|  1|1221|   5.0|\n",
      "|  1|1246|   5.0|\n",
      "+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghousu/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|   id|inDegree|\n",
      "+-----+--------+\n",
      "| 1645|   14346|\n",
      "| 1591|    6317|\n",
      "| 3175|   16216|\n",
      "| 1580|   42193|\n",
      "|68135|    2478|\n",
      "+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|outDegree|\n",
      "+---+---------+\n",
      "|148|       43|\n",
      "|463|      261|\n",
      "|471|      193|\n",
      "|496|       10|\n",
      "|833|        3|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 16:48:33 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=====================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 16:48:47 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 16:48:48 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|    id|outDegree|\n",
      "+------+---------+\n",
      "| 45811|    18276|\n",
      "|  8659|     9279|\n",
      "|270123|     7638|\n",
      "|179792|     7515|\n",
      "|228291|     7410|\n",
      "+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=====================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|  id|inDegree|\n",
      "+----+--------+\n",
      "| 356|   91921|\n",
      "| 318|   91082|\n",
      "| 296|   87901|\n",
      "| 593|   84078|\n",
      "|2571|   77960|\n",
      "+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Prepare the vertices DataFrame\n",
    "users_vertices = ra_id_rating.select(\"userId\").distinct().withColumnRenamed(\"userId\", \"id\")\n",
    "movies_vertices = ra_id_rating.select(\"movieId\").distinct().withColumnRenamed(\"movieId\", \"id\")\n",
    "\n",
    "vertices = users_vertices.union(movies_vertices)\n",
    "\n",
    "# Prepare the edges DataFrame\n",
    "edges = ra_id_rating.select(\"userId\", \"movieId\", \"rating\").withColumnRenamed(\"userId\", \"src\")\\\n",
    "                    .withColumnRenamed(\"movieId\", \"dst\")\n",
    "\n",
    "# Create a GraphFrame\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "# Show the vertices and edges\n",
    "graph.vertices.show(5)\n",
    "graph.edges.show(5)\n",
    "\n",
    "# Calculate the in-degree of each vertex\n",
    "in_degrees = graph.inDegrees\n",
    "in_degrees.show(5)\n",
    "\n",
    "# Calculate the out-degree of each vertex\n",
    "out_degrees = graph.outDegrees\n",
    "out_degrees.show(5)\n",
    "\n",
    "# Find the top 5 users who have rated the most movies\n",
    "top_raters = out_degrees.filter(col(\"id\").isin(users_vertices.select(\"id\").rdd.map(lambda row: row.id).collect()))\\\n",
    "                        .orderBy(\"outDegree\", ascending=False)\\\n",
    "                        .limit(5)\n",
    "top_raters.show()\n",
    "\n",
    "# Find the top 5 movies with the most ratings\n",
    "top_rated_movies = in_degrees.filter(col(\"id\").isin(movies_vertices.select(\"id\").rdd.map(lambda row: row.id).collect()))\\\n",
    "                             .orderBy(\"inDegree\", ascending=False)\\\n",
    "                             .limit(5)\n",
    "top_rated_movies.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "472fad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghousu/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|    id|\n",
      "+------+\n",
      "|110081|\n",
      "|109068|\n",
      "|117500|\n",
      "| 94265|\n",
      "|106544|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-----+\n",
      "|   src|  dst|\n",
      "+------+-----+\n",
      "|114709|  862|\n",
      "|113497| 8844|\n",
      "|113228|15602|\n",
      "|114885|31357|\n",
      "|113041|11862|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghousu/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    id|degree|\n",
      "+------+------+\n",
      "|110081|     1|\n",
      "|109068|     1|\n",
      "| 11858|     1|\n",
      "| 36355|     1|\n",
      "|117500|     2|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "# Prepare the vertices DataFrame\n",
    "imdb_vertices = links_df.select(\"imdbId\").distinct().withColumnRenamed(\"imdbId\", \"id\")\n",
    "tmdb_vertices = links_df.select(\"tmdbId\").distinct().withColumnRenamed(\"tmdbId\", \"id\")\n",
    "\n",
    "vertices = imdb_vertices.union(tmdb_vertices)\n",
    "\n",
    "# Prepare the edges DataFrame\n",
    "edges = links_df.select(\"imdbId\", \"tmdbId\").withColumnRenamed(\"imdbId\", \"src\").withColumnRenamed(\"tmdbId\", \"dst\")\n",
    "\n",
    "# Create a GraphFrame\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "# Show the vertices and edges\n",
    "graph.vertices.show(5)\n",
    "graph.edges.show(5)\n",
    "\n",
    "# Calculate the degree of each vertex\n",
    "degrees = graph.degrees\n",
    "degrees.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e906762",
   "metadata": {},
   "source": [
    "This result shows some basic information about the GraphFrame created using the links_df dataset. We can see that the vertices DataFrame is only showing the top 5 vertices, which includes the imdbId data. The edges DataFrame displays the top 5 mappings between imdbId and tmdbId, i.e., the source vertex (src) and the target vertex (dst).\n",
    "\n",
    "Finally, we calculated the degree of each vertex, and here we are showing the top 5 vertices' degrees. The degree represents the number of edges related to the vertex. In this example, we can see that the degree varies between 1 and 2. This indicates that in our dataset, the association between most imdbId and tmdbId is relatively weak, with only a few imdbId being associated with multiple tmdbId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e83f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139b49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf3eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c756b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
